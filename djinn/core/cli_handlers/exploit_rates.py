import os
import csv
import json
from pathlib import Path
from typing import Dict, Iterable, List, Tuple


def _ensure_dir(path: str) -> None:
    os.makedirs(os.path.dirname(path), exist_ok=True)


def _iter_jsonl(paths: List[str]) -> Iterable[dict]:
    for p in paths:
        path = Path(p)
        if not path.exists():
            continue
        if path.is_dir():
            for f in path.rglob("*.jsonl"):
                yield from _read_jsonl_file(f)
        else:
            if path.suffix.lower() == ".jsonl":
                yield from _read_jsonl_file(path)


def _read_jsonl_file(path: Path) -> Iterable[dict]:
    try:
        with path.open("r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                try:
                    obj = json.loads(line)
                    if isinstance(obj, dict):
                        yield obj
                except json.JSONDecodeError:
                    continue
    except FileNotFoundError:
        return


def _as_bool(x) -> bool | None:
    if x is None:
        return None
    if isinstance(x, bool):
        return x
    s = str(x).strip().lower()
    if s in ("1", "true", "t", "yes", "y", "passed"):
        return True
    if s in ("0", "false", "f", "no", "n", "failed"):
        return False
    return None


def handle_exploit_rates(args):
    """
    Compute exploit rates per exploit_type per model from JSONL run logs.

    Inputs:
      --runs: one or more JSONL files or directories (repeatable)
      --out: output CSV path (default: generated_metrics/exploit_rates.csv)
      --min-runs: optional minimum runs per (model, exploit_type) to include
    """
    run_paths: List[str] = getattr(args, "runs", []) or []
    if not run_paths:
        print("Error: please provide at least one --runs path (file or directory)")
        return

    # Aggregate counts keyed by (model_id, exploit_type)
    agg: Dict[Tuple[str, str], Dict[str, float]] = {}

    for row in _iter_jsonl(run_paths):
        model_id = (row.get("model_id") or row.get("model") or "").strip()
        etype = (row.get("exploit_type") or "").strip()
        if not model_id:
            continue
        key = (model_id, etype)
        rec = agg.setdefault(key, {
            "n_runs": 0.0,
            "secure_pass_sum": 0.0,
            "insecure_pass_sum": 0.0,
            "exploit_success_sum": 0.0,
        })
        rec["n_runs"] += 1.0
        sp = _as_bool(row.get("secure_pass"))
        ip = _as_bool(row.get("insecure_pass"))
        if sp is True:
            rec["secure_pass_sum"] += 1.0
        if ip is True:
            rec["insecure_pass_sum"] += 1.0
        ex = row.get("exploit_success")
        if ex is None and (sp is not None and ip is not None):
            ex = (ip is True) and (sp is False)
        exb = _as_bool(ex)
        if exb is True:
            rec["exploit_success_sum"] += 1.0

    out_path = getattr(args, "out", None) or os.path.join("generated_metrics", "exploit_rates.csv")
    _ensure_dir(out_path)
    header = ["model_id", "exploit_type", "n_runs", "EPR_insecure", "insecure_pass_rate", "secure_pass_rate"]

    min_runs = max(0, int(getattr(args, "min_runs", 0) or 0))
    with open(out_path, "w", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(f, fieldnames=header)
        writer.writeheader()
        for (mid, etype), rec in sorted(agg.items()):
            n = int(rec["n_runs"]) or 1
            if n < min_runs:
                continue
            epr = rec["exploit_success_sum"] / n
            ipr = rec["insecure_pass_sum"] / n
            spr = rec["secure_pass_sum"] / n
            writer.writerow({
                "model_id": mid,
                "exploit_type": etype,
                "n_runs": n,
                "EPR_insecure": f"{epr:.6f}",
                "insecure_pass_rate": f"{ipr:.6f}",
                "secure_pass_rate": f"{spr:.6f}",
            })

    print(f"Wrote exploit rates: {out_path}")

