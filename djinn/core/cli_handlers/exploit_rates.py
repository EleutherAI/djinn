import os
import csv
import json
from pathlib import Path
from typing import Dict, Iterable, List, Tuple


def _ensure_dir(path: str) -> None:
    os.makedirs(os.path.dirname(path), exist_ok=True)


def _iter_jsonl(paths: List[str]) -> Iterable[dict]:
    for p in paths:
        path = Path(p)
        if not path.exists():
            continue
        if path.is_dir():
            for f in path.rglob("*.jsonl"):
                yield from _read_jsonl_file(f)
        else:
            if path.suffix.lower() == ".jsonl":
                yield from _read_jsonl_file(path)


def _read_jsonl_file(path: Path) -> Iterable[dict]:
    try:
        with path.open("r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                try:
                    obj = json.loads(line)
                    if isinstance(obj, dict):
                        yield obj
                except json.JSONDecodeError:
                    continue
    except FileNotFoundError:
        return


def _discover_model_files(root_dir: str) -> List[Tuple[str, Path]]:
    """
    Scan a directory recursively for per-model JSONL files matching the desired
    suffixes. Returns a list of (model_label, path) where model_label is derived
    from the file name (without extension), e.g., 'gpt_oss_120b_base'.
    """
    base = Path(root_dir)
    if not base.exists() or not base.is_dir():
        return []

    patterns = [
        "*_base.jsonl",
        "*_ft.jsonl",
        "*_base_noprompt.jsonl",
        "*_ft_noprompt.jsonl",
        # Train split variants
        "*_base_train.jsonl",
        "*_ft_train.jsonl",
        "*_base_train_noprompt.jsonl",
        "*_ft_train_noprompt.jsonl",
        "*_ft_train_noprompt.jsonl",
    ]

    results: List[Tuple[str, Path]] = []
    for pat in patterns:
        for f in base.rglob(pat):
            # Exclude any sample/helper files just in case
            if str(f).endswith(".samples.jsonl"):
                continue
            results.append((f.stem, f))
    return results


def _as_bool(x) -> bool | None:
    if x is None:
        return None
    if isinstance(x, bool):
        return x
    s = str(x).strip().lower()
    if s in ("1", "true", "t", "yes", "y", "passed"):
        return True
    if s in ("0", "false", "f", "no", "n", "failed"):
        return False
    return None


def _parse_label_and_split(label: str) -> Tuple[str, str]:
    """
    Parse a filename stem like 'gpt_oss_120b_ft', 'gpt_oss_120b_ft_noprompt',
    'gpt_oss_120b_ft_train', or 'gpt_oss_120b_ft_train_noprompt' into a
    canonical (model_id, split) pair where split is one of {"train", "eval"} and
    model_id excludes the optional '_train' token.
    """
    # Simple token-based normalization to avoid heavy regex and edge cases
    parts = label.split("_")
    split = "eval"
    if "train" in parts:
        split = "train"
        parts = [p for p in parts if p != "train"]
    # Nothing else to normalize; keep possible trailing 'noprompt'
    canonical = "_".join(parts)
    return canonical, split


def handle_exploit_rates(args):
    """
    Compute exploit rates per exploit_type per model from JSONL run logs.

    Inputs:
      --runs: one or more JSONL files or directories (repeatable)
      --out: output CSV path (default: generated_metrics/exploit_rates.csv)
      --min-runs: optional minimum runs per (model, exploit_type) to include
    """
    dir_path: str | None = getattr(args, "dir", None)
    run_paths: List[str] = getattr(args, "runs", []) or []
    labeled_files: List[Tuple[str, Path]] = []

    if dir_path:
        labeled_files = _discover_model_files(dir_path)
        if not labeled_files:
            print(f"Warning: no matching JSONL files found under --dir={dir_path}")
    elif not run_paths:
        print("Error: please provide at least one --runs path (file or directory) or --dir")
        return

    # Aggregate counts keyed by (model_id, split, exploit_type)
    agg: Dict[Tuple[str, str, str], Dict[str, float]] = {}

    if labeled_files:
        # Directory mode: treat each matching file as a distinct model, labeled by filename
        for label, fpath in labeled_files:
            canonical_model_id, split = _parse_label_and_split(label)
            for row in _read_jsonl_file(fpath):
                model_id = canonical_model_id
                etype = (row.get("exploit_type") or "").strip()
                key = (model_id, split, etype)
                rec = agg.setdefault(key, {
                    "n_runs": 0.0,
                    "secure_pass_sum": 0.0,
                    "insecure_pass_sum": 0.0,
                    "exploit_success_sum": 0.0,
                })
                rec["n_runs"] += 1.0
                sp = _as_bool(row.get("secure_pass"))
                ip = _as_bool(row.get("insecure_pass"))
                if sp is True:
                    rec["secure_pass_sum"] += 1.0
                if ip is True:
                    rec["insecure_pass_sum"] += 1.0
                ex = row.get("exploit_success")
                if ex is None and (sp is not None and ip is not None):
                    ex = (ip is True) and (sp is False)
                exb = _as_bool(ex)
                if exb is True:
                    rec["exploit_success_sum"] += 1.0
    else:
        # Legacy mode: read rows and use their embedded model_id
        for row in _iter_jsonl(run_paths):
            model_id = (row.get("model_id") or row.get("model") or "").strip()
            etype = (row.get("exploit_type") or "").strip()
            split = (row.get("split") or "eval").strip() or "eval"
            if not model_id:
                continue
            # If the embedded model_id contains a '_train' token, normalize it out and set split
            normalized_model_id, inferred_split = _parse_label_and_split(model_id)
            if inferred_split == "train":
                split = "train"
            model_id = normalized_model_id
            key = (model_id, split, etype)
            rec = agg.setdefault(key, {
                "n_runs": 0.0,
                "secure_pass_sum": 0.0,
                "insecure_pass_sum": 0.0,
                "exploit_success_sum": 0.0,
            })
            rec["n_runs"] += 1.0
            sp = _as_bool(row.get("secure_pass"))
            ip = _as_bool(row.get("insecure_pass"))
            if sp is True:
                rec["secure_pass_sum"] += 1.0
            if ip is True:
                rec["insecure_pass_sum"] += 1.0
            ex = row.get("exploit_success")
            if ex is None and (sp is not None and ip is not None):
                ex = (ip is True) and (sp is False)
            exb = _as_bool(ex)
            if exb is True:
                rec["exploit_success_sum"] += 1.0

    if getattr(args, "out", None):
        out_path = args.out
    elif dir_path:
        out_path = os.path.join(dir_path, "exploit_rates.csv")
    else:
        out_path = os.path.join("generated_metrics", "exploit_rates.csv")
    _ensure_dir(out_path)
    header = ["model_id", "split", "exploit_type", "n_runs", "EPR_insecure", "insecure_pass_rate", "secure_pass_rate"]

    min_runs = max(0, int(getattr(args, "min_runs", 0) or 0))
    with open(out_path, "w", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(f, fieldnames=header)
        writer.writeheader()
        for (mid, split, etype), rec in sorted(agg.items()):
            n = int(rec["n_runs"]) or 1
            if n < min_runs:
                continue
            epr = rec["exploit_success_sum"] / n
            ipr = rec["insecure_pass_sum"] / n
            spr = rec["secure_pass_sum"] / n
            writer.writerow({
                "model_id": mid,
                "split": split,
                "exploit_type": etype,
                "n_runs": n,
                "EPR_insecure": f"{epr:.6f}",
                "insecure_pass_rate": f"{ipr:.6f}",
                "secure_pass_rate": f"{spr:.6f}",
            })

    print(f"Wrote exploit rates: {out_path}")

